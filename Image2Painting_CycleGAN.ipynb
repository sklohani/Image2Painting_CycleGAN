{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport sys\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport glob\nimport random\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport copy\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nfrom tqdm import tqdm\nimport random\nimport shutil","metadata":{"_uuid":"f9ee07c9-03d4-4b61-9705-e4ceea4629d9","_cell_guid":"c53fa6a7-0971-4032-9649-189786466278","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-10T12:40:46.515990Z","iopub.execute_input":"2023-05-10T12:40:46.516323Z","iopub.status.idle":"2023-05-10T12:40:51.955056Z","shell.execute_reply.started":"2023-05-10T12:40:46.516288Z","shell.execute_reply":"2023-05-10T12:40:51.953989Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# def histogram_equalization(img):\n#     img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n#     img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n#     img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n#     return img_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    transforms.ToTensor(),\n])\"\"\"","metadata":{"_uuid":"0525aa57-53a0-4707-b71c-e553ad4befc6","_cell_guid":"3eff25c9-af2f-4d56-a977-25c6e4fab064","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def preprocess_imgs(img_path):\n#     img = cv2.imread(img_path)\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n#     img = histogram_equalization(img)\n#     img = transform(img)\n    \n#     return img","metadata":{"_uuid":"0cbe3b6c-ebac-45b0-a305-56c04fe97002","_cell_guid":"d09b5c3c-dcbd-4a1a-883e-9221b1e059f4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img_path = \"/kaggle/input/best-artworks-of-all-time/images/images/Vincent_van_Gogh/Vincent_van_Gogh_109.jpg\"\n# processed_img = preprocess_imgs(img_path)\n# img = transforms.ToPILImage()(processed_img)","metadata":{"_uuid":"144717af-7704-4469-9ff1-4d138a6d317f","_cell_guid":"7ea5d952-4e1e-4e8c-8b97-038fae9feac1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Normal Min Max Normalization**","metadata":{}},{"cell_type":"code","source":"# def norm_func(min_val, max_val):\n#     transform = transforms.Compose([\n#         transforms.Normalize((min_val,), (max_val-min_val,))\n#     ])\n    \n#     return transform\n\n# def transf(img, processed_img):\n#     min_val = np.min(img)\n#     max_val = np.max(img)\n    \n#     transform1 = norm_func(min_val, max_val)\n#     normalized_img = transform1(processed_img)\n    \n#     return normalized_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output_img = np.array(transf(img, processed_img))\n\n# plt.hist(output_img.ravel(), bins=50, density=True)\n# plt.xlabel(\"pixel values\")\n# plt.ylabel(\"relative frequency\")\n# plt.title(\"distribution of pixels\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Generator**","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, use_activation=True, **kwargs):\n        super().__init__()\n\n        self.conv = nn.Sequential(\n            \n            nn.Conv2d(\n                in_channels = in_channels,\n                out_channels = out_channels,\n                padding_mode = \"reflect\",\n                **kwargs\n            ) if down \n            else nn.ConvTranspose2d(\n                in_channels = in_channels,\n                out_channels = out_channels,\n                **kwargs),\n\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace = True) if use_activation else nn.Identity(),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            ConvBlock(channels, channels, kernel_size=3, stride = 1, padding = 1),\n            ConvBlock(channels, channels, use_activation=False, kernel_size=3, stride = 1, padding = 1),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass Generator(nn.Module):\n    \n    def __init__(self, img_channels, num_features = 64, num_residuals = 9):\n        super().__init__()\n        \n        self.initial = nn.Sequential(\n            nn.Conv2d(\n                in_channels = img_channels,\n                out_channels = num_features,\n                kernel_size = 7,\n                stride = 1,\n                padding = 3,\n                padding_mode = \"reflect\",\n            ),\n            nn.ReLU(inplace = True),\n        )\n\n        self.down_blocks = nn.ModuleList(\n            [\n              ConvBlock(\n                  in_channels = num_features,\n                  out_channels = 2*num_features,\n                  kernel_size = 3,\n                  stride = 2,\n                  padding = 1,\n                  down = True,\n              ),\n             ConvBlock(\n                    in_channels = 2*num_features,\n                    out_channels = 4*num_features,\n                    kernel_size = 3,\n                    stride = 2,\n                    padding = 1,\n                    down = True,\n              ),\n            ]\n        )\n\n        self.residual_block = nn.Sequential(\n            *[ResidualBlock(4*num_features) for _ in range(num_residuals)]\n        )\n\n        self.up_blocks = nn.ModuleList(\n            [\n              ConvBlock(\n                  in_channels = 4*num_features,\n                  out_channels = 2*num_features,\n                  kernel_size = 3,\n                  stride = 2,\n                  padding = 1,\n                  output_padding = 1,\n                  down = False,\n              ),\n             ConvBlock(\n                    in_channels = 2*num_features,\n                    out_channels = num_features,\n                    kernel_size = 3,\n                    stride = 2,\n                    padding = 1,\n                    output_padding = 1,\n                    down = False,\n              ),\n            ]\n        )\n\n        self.last = nn.Conv2d(\n                in_channels = num_features,\n                out_channels = img_channels,\n                kernel_size = 7,\n                stride = 1,\n                padding = 3,\n                padding_mode = \"reflect\",\n        )\n\n    def forward(self, x):\n        x = self.initial(x)\n        for layer in self.down_blocks:\n            x = layer(x)\n        x = self.residual_block(x)\n        for layer in self.up_blocks:\n            x = layer(x)\n        x = self.last(x)\n        return torch.tanh(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:40:57.198852Z","iopub.execute_input":"2023-05-10T12:40:57.199467Z","iopub.status.idle":"2023-05-10T12:40:57.223046Z","shell.execute_reply.started":"2023-05-10T12:40:57.199425Z","shell.execute_reply":"2023-05-10T12:40:57.221941Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Generator**\n* 6 residual blocks for 128x128 training image\n* 9 residual blocks for 256x256 or higher resolution training image\n* C7s1-k -> 7x7 Convolution-InstanceNorm-ReLU layer with k-filters and stride 1\n* dk -> 3x3 Convolution-InstanceNorm-ReLU layer with k-filters and stride 2\n* uk-> 3x3 Convolution-InstanceNorm-ReLU layer with k-filters and stride Â½ \n* Rk -> Residual block contains 3x3 Convolution layer with same no. of filters on both layers\n* Network with 6 residual blocks\n* C7s1-32, d64, d128, 6*(R128), u64, u32, C7s1-3\n* Network with 9 residual blocks\n* C7s1-32, d64, d128, 9*(R128), u64, u32, C7s1-3\n\n![Generator](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVBSmRcCz9xfw-fCNi_q5g.png)","metadata":{}},{"cell_type":"markdown","source":"# **Discriminator**","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias = True, padding_mode=\"reflect\"),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.2),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, features=[64, 128, 256,512]):\n        super().__init__()\n    \n        self.initial = nn.Sequential(\n            nn.Conv2d(\n                in_channels = in_channels,\n                out_channels = features[0],\n                kernel_size = 4,\n                stride = 2,\n                padding = 1,\n                padding_mode = \"reflect\",\n            ),\n            nn.LeakyReLU(0.2),\n        )\n\n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:\n            stride = 1 if feature == features[-1] else 2\n            layers.append(Block(in_channels, feature, stride))\n            in_channels = feature\n    \n        layers.append(\n            nn.Conv2d(\n                in_channels = in_channels,\n                out_channels =  1,\n                kernel_size = 4,\n                stride = 1,\n                padding = 1,\n                padding_mode = \"reflect\",\n            )\n        )\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.initial(x)\n        x = self.model(x)\n        return torch.sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:41:01.069224Z","iopub.execute_input":"2023-05-10T12:41:01.070331Z","iopub.status.idle":"2023-05-10T12:41:01.083726Z","shell.execute_reply.started":"2023-05-10T12:41:01.070261Z","shell.execute_reply":"2023-05-10T12:41:01.082422Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Discriminator**\n* 70x70 PatchGAN\n* Ck -> 4x4 Convolution-InstanceNorm-ReLU layer with k-filters and stride 2\n* Discriminator Network\n* C64, C128, C256, C512\n* After the last layer, apply a Convolution to produce 1-dimensional output\n* Do not use InstanceNorm for the first C64 layer\n* Use Leaky-ReLU with slope 0.2\n\n![Discriminator](https://miro.medium.com/v2/resize:fit:828/format:webp/1*46CddTc5JwkFW_pQb4nGZQ.png)","metadata":{}},{"cell_type":"markdown","source":"**config.py**","metadata":{}},{"cell_type":"code","source":"def get_X_domain(x_path, n, m = 200):    \n    if not os.path.exists('x_images'):\n        os.makedirs('x_images')\n#     if not os.path.exists('test_dir'):\n#         os.makedirs('test_dir')\n        \n    x_images = os.listdir(x_path)\n    x_len = len(x_images)\n    \n    for i in range(n):\n        img = random.choice(x_images)\n        shutil.copy(x_path + \"/\" + img, 'x_images' + \"/\" + f'x_{i}')\n        if i%100 == 0: print(i)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:41:06.943974Z","iopub.execute_input":"2023-05-10T12:41:06.944435Z","iopub.status.idle":"2023-05-10T12:41:06.954338Z","shell.execute_reply.started":"2023-05-10T12:41:06.944387Z","shell.execute_reply":"2023-05-10T12:41:06.953323Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"get_X_domain(\"/kaggle/input/gan-getting-started/photo_jpg\", 1350)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:41:09.165528Z","iopub.execute_input":"2023-05-10T12:41:09.166115Z","iopub.status.idle":"2023-05-10T12:41:17.767231Z","shell.execute_reply.started":"2023-05-10T12:41:09.166074Z","shell.execute_reply":"2023-05-10T12:41:17.765961Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n","output_type":"stream"}]},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# TRAIN_DIR_X = \"/kaggle/input/images-r/train_set\"\nTRAIN_DIR_X = \"/kaggle/working/x_images\"\n# TRAIN_DIR_Y = \"/kaggle/input/images/dataset/y_domain\"\nTRAIN_DIR_Y = \"/kaggle/input/best-artworks-of-all-time/images/images/Vincent_van_Gogh/\"\n# VAL_DIR = \"/kaggle/input/images/dataset/x_domain/test_dir\"\nBATCH_SIZE = 1\nLEARNING_RATE = 2e-4\nLAMBDA_IDENTITY = 5.0\nLAMBDA_CYCLE = 10\nNUM_WORKERS = 2\nNUM_EPOCHS = 50\nLOAD_MODEL = True\nSAVE_MODEL = True\nCHECKPOINT_GEN_X = \"genx.pth.tar\"\nCHECKPOINT_GEN_Y = \"geny.pth.tar\"\nCHECKPOINT_CRITIC_X = \"criticx.pth.tar\"\nCHECKPOINT_CRITIC_Y = \"criticy.pth.tar\"\nLOAD_CHECKPOINT_GEN_X = \"/kaggle/input/models/genx.pth.tar\"\nLOAD_CHECKPOINT_GEN_Y = \"/kaggle/input/models/geny.pth.tar\"\nLOAD_CHECKPOINT_CRITIC_X = \"/kaggle/input/models/criticx.pth.tar\"\nLOAD_CHECKPOINT_CRITIC_Y = \"/kaggle/input/models/criticy.pth.tar\"\n\n_transforms = A.Compose(\n    [\n        A.Resize(width=256, height=256),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n        ToTensorV2(),\n    ],\n    additional_targets={\"image0\": \"image\"},\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:45:01.802352Z","iopub.execute_input":"2023-05-10T12:45:01.803093Z","iopub.status.idle":"2023-05-10T12:45:01.812922Z","shell.execute_reply.started":"2023-05-10T12:45:01.803053Z","shell.execute_reply":"2023-05-10T12:45:01.811810Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**utils.py**","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"==> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n    \ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"==> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    \n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n        \ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:41:53.393293Z","iopub.execute_input":"2023-05-10T12:41:53.394347Z","iopub.status.idle":"2023-05-10T12:41:53.404544Z","shell.execute_reply.started":"2023-05-10T12:41:53.394271Z","shell.execute_reply":"2023-05-10T12:41:53.403438Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**dataset.py**","metadata":{}},{"cell_type":"code","source":"class XYDataset(Dataset):\n    def __init__(self, root_y, root_x, transform=None):\n        self.root_y = root_y\n        self.root_x = root_x\n        self.transform = transform\n        \n        self.y_images = os.listdir(root_y)\n        self.x_images = os.listdir(root_x)\n        self.length_dataset = max(len(self.y_images), len(self.x_images)) # 877 4947\n        self.y_len = len(self.y_images)\n        self.x_len = len(self.x_images)\n        \n    def __len__(self):\n        return self.length_dataset\n    \n    def __getitem__(self, index):\n        y_img = self.y_images[index % self.y_len]\n        x_img = self.x_images[index % self.x_len]\n        \n        y_path = os.path.join(self.root_y , y_img)\n        x_path = os.path.join(self.root_x , x_img)\n        \n        y_img = np.array(Image.open(y_path).convert(\"RGB\"))\n        x_img = np.array(Image.open(x_path).convert(\"RGB\"))\n        \n        if self.transform:\n            augmentations = self.transform(image = y_img, image0 = x_img)\n            y_img = augmentations[\"image\"]\n            x_img = augmentations[\"image0\"]\n            \n        return y_img, x_img","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:41:55.972354Z","iopub.execute_input":"2023-05-10T12:41:55.973045Z","iopub.status.idle":"2023-05-10T12:41:55.985072Z","shell.execute_reply.started":"2023-05-10T12:41:55.973006Z","shell.execute_reply":"2023-05-10T12:41:55.983847Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**train.py**","metadata":{}},{"cell_type":"code","source":"def train_fn(disc_X, disc_Y, gen_Y, gen_X, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n    X_reals = 0\n    X_fakes = 0\n    loop = tqdm(loader, leave = True)\n    \n    for idx, (y, x) in enumerate(loop):\n        y = y.to(DEVICE)\n        x = x.to(DEVICE)\n        \n        # Discriminator\n        with torch.cuda.amp.autocast():\n            fake_x = gen_X(y)\n            D_X_real = disc_X(x)\n            D_X_fake = disc_X(fake_x.detach())\n            X_reals += D_X_real.mean().item()\n            X_fakes += D_X_fake.mean().item()\n            D_X_real_loss = mse(D_X_real, torch.ones_like(D_X_real))\n            D_X_fake_loss = mse(D_X_fake, torch.zeros_like(D_X_fake))\n            D_X_loss = D_X_real_loss + D_X_fake_loss\n            \n            fake_y = gen_Y(x)\n            D_Y_real = disc_Y(y)\n            D_Y_fake = disc_Y(fake_y.detach())\n            D_Y_real_loss = mse(D_Y_real, torch.ones_like(D_Y_real))\n            D_Y_fake_loss = mse(D_Y_fake, torch.zeros_like(D_Y_fake))\n            D_Y_loss = D_Y_real_loss + D_Y_fake_loss\n            \n            D_loss = (D_X_loss + D_Y_loss) / 2\n            \n        opt_disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        \n        # Generators\n        with torch.cuda.amp.autocast():\n            # adversarial loss\n            D_X_fake = disc_X(fake_x)\n            D_Y_fake = disc_Y(fake_y)\n            \n            loss_G_X = mse(D_X_fake, torch.ones_like(D_X_fake))\n            loss_G_Y = mse(D_Y_fake, torch.ones_like(D_Y_fake))\n            \n            # cycle loss\n            cycle_y = gen_Y(fake_x)\n            cycle_x = gen_X(fake_y)\n            cycle_y_loss = l1(y, cycle_y)\n            cycle_x_loss = l1(x, cycle_x)\n            \n            # identity loss\n            identity_y = gen_Y(y)\n            identity_x = gen_X(x)\n            identity_y_loss = l1(y, identity_y)\n            identity_x_loss = l1(x, identity_x)\n            \n            # add all together\n            G_loss = (\n                loss_G_Y \n                + loss_G_X \n                + cycle_y_loss * LAMBDA_CYCLE \n                + cycle_x_loss * LAMBDA_CYCLE\n                + identity_x_loss * LAMBDA_IDENTITY \n                + identity_y_loss * LAMBDA_IDENTITY\n            )\n            \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n\n        if idx % NUM_EPOCHS == 0:\n            save_image(x * 0.5 + 0.5, f\"saved_images/x_{idx}.png\")\n            save_image(fake_y * 0.5 + 0.5, f\"saved_images/fake_y_{idx}.png\")\n\n        loop.set_postfix(X_real = X_reals / (idx + 1), X_fake = X_fakes / (idx + 1))\n            \ndef main():\n#     if not os.path.exists('cyclegan_test'):\n#         os.makedirs('cyclegan_test')\n    \n#     if not os.path.exists('cyclegan_test/x'):\n#         os.makedirs('cyclegan_test/x')\n    \n#     if not os.path.exists('cyclegan_test/y'):\n#         os.makedirs('cyclegan_test/y')\n        \n    if not os.path.exists('saved_images'):\n        os.makedirs('saved_images')\n    disc_X = Discriminator(in_channels = 3).to(DEVICE)\n    disc_Y = Discriminator(in_channels = 3).to(DEVICE)\n    gen_Y = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n    gen_X = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n    opt_disc = optim.Adam(\n        list(disc_X.parameters()) + list(disc_Y.parameters()),\n        lr = LEARNING_RATE,\n        betas = (0.5, 0.999),\n    )\n    \n    opt_gen = optim.Adam(\n        list(gen_X.parameters()) + list(gen_Y.parameters()),\n        lr = LEARNING_RATE,\n        betas = (0.5, 0.999),\n    )\n    \n    L1 = nn.L1Loss()   # CycleConsistency & Identity\n    mse = nn.MSELoss() # Adversial\n    \n    if LOAD_MODEL:\n        load_checkpoint(\n            LOAD_CHECKPOINT_GEN_X,\n            gen_X,\n            opt_gen,\n            LEARNING_RATE,\n        )\n        load_checkpoint(\n            LOAD_CHECKPOINT_GEN_Y,\n            gen_Y,\n            opt_gen,\n            LEARNING_RATE,\n        )\n        load_checkpoint(\n            LOAD_CHECKPOINT_CRITIC_X,\n            disc_X,\n            opt_disc,\n            LEARNING_RATE,\n        )\n        load_checkpoint(\n            LOAD_CHECKPOINT_CRITIC_Y,\n            disc_Y,\n            opt_disc,\n            LEARNING_RATE,\n        )\n    \n    dataset = XYDataset(\n        root_x = TRAIN_DIR_X,\n        root_y = TRAIN_DIR_Y,\n        transform = _transforms,\n    )\n    \n#     val_dataset = XYDataset(\n#         root_x = \"cyclegan_test/x\",\n#         root_y = \"cyclegan_test/y\",\n#         transform = _transforms,\n#     )\n    \n#     val_loader = DataLoader(\n#         val_dataset,\n#         batch_size = 1,\n#         shuffle = False,\n#         pin_memory = True,\n#     )\n\n    loader = DataLoader(\n        dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n    g_scaler = torch.cuda.amp.GradScaler()\n    d_scaler = torch.cuda.amp.GradScaler()\n    \n    for epoch in range(NUM_EPOCHS):\n        print(f\"===> Epoch: {epoch+1}\")\n        train_fn(\n            disc_X,\n            disc_Y,\n            gen_Y,\n            gen_X,\n            loader,\n            opt_disc,\n            opt_gen,\n            L1,\n            mse,\n            d_scaler,\n            g_scaler,\n        )\n        \n        if SAVE_MODEL:\n            save_checkpoint(gen_X, opt_gen, filename = CHECKPOINT_GEN_X)\n            save_checkpoint(gen_Y, opt_gen, filename = CHECKPOINT_GEN_Y)\n            save_checkpoint(disc_X, opt_disc, filename = CHECKPOINT_CRITIC_X)\n            save_checkpoint(disc_Y, opt_disc, filename = CHECKPOINT_CRITIC_Y)\n            \n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:45:10.302953Z","iopub.execute_input":"2023-05-10T12:45:10.303853Z","iopub.status.idle":"2023-05-10T12:45:10.333629Z","shell.execute_reply.started":"2023-05-10T12:45:10.303810Z","shell.execute_reply":"2023-05-10T12:45:10.332463Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ls -lA","metadata":{"execution":{"iopub.status.busy":"2023-05-10T12:25:03.173132Z","iopub.execute_input":"2023-05-10T12:25:03.173878Z","iopub.status.idle":"2023-05-10T12:25:04.194188Z","shell.execute_reply.started":"2023-05-10T12:25:03.173838Z","shell.execute_reply":"2023-05-10T12:25:04.192958Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"total 8\ndrwxr-xr-x 2 root root 4096 May 10 12:22 .virtual_documents\n---------- 1 root root  263 May 10 12:22 __notebook_source__.ipynb\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}